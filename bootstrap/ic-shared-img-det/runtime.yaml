# ---
# apiVersion: serving.kserve.io/v1alpha1
# kind: ServingRuntime
# metadata:
#   annotations:
#     enable-auth: "false"
#     enable-route: "false"
#     maxLoadingConcurrency: "2"
#     opendatahub.io/template-display-name: Triton runtime 23.11
#     opendatahub.io/template-name: triton-23.11-20231217
#     openshift.io/display-name: "Triton 23.11 - added on 20231217"
#     argocd.argoproj.io/sync-wave: "3"
#     argocd.argoproj.io/sync-options: SkipDryRunOnMissingResource=true
#   labels:
#     name: triton
#     opendatahub.io/dashboard: "true"
#   name: triton
# spec:
#   builtInAdapter:
#     memBufferBytes: 134217728
#     modelLoadingTimeoutMillis: 90000
#     runtimeManagementPort: 8001
#     serverType: triton
#   containers:
#   - args:
#     - -c
#     - 'mkdir -p /models/_triton_models; chmod 777 /models/_triton_models; exec tritonserver
#       "--model-repository=/models/_triton_models" "--model-control-mode=explicit"
#       "--strict-model-config=false" "--strict-readiness=false" "--allow-http=true"
#       "--allow-sagemaker=false" '
#     command:
#     - /bin/sh
#     image: nvcr.io/nvidia/tritonserver:23.11-py3
#     livenessProbe:
#       exec:
#         command:
#         - curl
#         - --fail
#         - --silent
#         - --show-error
#         - --max-time
#         - "9"
#         - http://localhost:8000/v2/health/live
#       initialDelaySeconds: 5
#       periodSeconds: 30
#       timeoutSeconds: 10
#     name: triton
#     resources:
#       limits:
#         cpu: "1"
#         memory: 8Gi
#       requests:
#         cpu: "1"
#         memory: 2Gi
#   grpcDataEndpoint: port:8001
#   grpcEndpoint: port:8085
#   multiModel: true
#   protocolVersions:
#   - grpc-v2
#   replicas: 1
#   supportedModelFormats:
#   - autoSelect: true
#     name: keras
#     version: "2"
#   - autoSelect: true
#     name: onnx
#     version: "1"
#   - autoSelect: true
#     name: pytorch
#     version: "1"
#   - autoSelect: true
#     name: tensorflow
#     version: "1"
#   - autoSelect: true
#     name: tensorflow
#     version: "2"
#   - autoSelect: true
#     name: tensorrt
#     version: "7"
---
apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  annotations:
    enable-route: 'true'
    opendatahub.io/accelerator-name: ''
    opendatahub.io/template-display-name: OpenVINO Model Server
    opendatahub.io/template-name: ovms
    openshift.io/display-name: ovms
  name: ovms
  namespace: ic-shared-img-det
  labels:
    name: ovms
    opendatahub.io/dashboard: 'true'
spec:
  supportedModelFormats:
    - autoSelect: true
      name: openvino_ir
      version: opset1
    - autoSelect: true
      name: onnx
      version: '1'
    - autoSelect: true
      name: tensorflow
      version: '2'
  builtInAdapter:
    env:
      - name: OVMS_FORCE_TARGET_DEVICE
        value: AUTO
    memBufferBytes: 134217728
    modelLoadingTimeoutMillis: 90000
    runtimeManagementPort: 8888
    serverType: ovms
  multiModel: true
  containers:
    - args:
        - '--port=8001'
        - '--rest_port=8888'
        - '--config_path=/models/model_config_list.json'
        - '--file_system_poll_wait_seconds=0'
        - '--grpc_bind_address=127.0.0.1'
        - '--rest_bind_address=127.0.0.1'
      image: >-
        quay.io/opendatahub/openvino_model_server@sha256:2cbe8a48ab0bc6fe7fb76919bf33253e83a6218a9c4b486b744c3dcf30679616
      name: ovms
      resources:
        limits:
          cpu: '1'
          memory: 1Gi
        requests:
          cpu: '1'
          memory: 1Gi
      volumeMounts:
        - mountPath: /dev/shm
          name: shm
  protocolVersions:
    - grpc-v1
  grpcEndpoint: 'port:8085'
  volumes:
    - emptyDir:
        medium: Memory
        sizeLimit: 2Gi
      name: shm
  replicas: 1
  tolerations: []
  grpcDataEndpoint: 'port:8001'
