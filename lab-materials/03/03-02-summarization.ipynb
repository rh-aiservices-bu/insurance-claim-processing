{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ad2cc4e-31ec-4648-b0fe-6632f2bdbc36",
   "metadata": {},
   "source": [
    "## Summarizing a text using an LLM\n",
    "\n",
    "As an LLM \"understands\" a language, it can be suited for tasks like translation or summarization.\n",
    "\n",
    "In this Notebook, we are going to use our LLM to summarize some texts, especially claims examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4e2b81-0e10-4390-a7b8-5ddfda53a3e3",
   "metadata": {},
   "source": [
    "### Requirements and Imports\n",
    "\n",
    "If you have selected the right workbench image to launch as per the Lab's instructions, you should already have all the needed libraries. If not uncomment the first line in the next cell to install all the right packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61c595d-967e-47de-a598-02b5d1ccec85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install --no-cache-dir --no-dependencies --disable-pip-version-check -r requirements.txt # Uncomment only if you have not selected the right workbench image\n",
    "\n",
    "import json\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_community.llms import VLLMOpenAI\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c428fbad-2345-4536-b687-72416d6b9b15",
   "metadata": {},
   "source": [
    "### Langchain pipeline\n",
    "\n",
    "Again, we are going to use Langchain to define our summarization pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f95a70-89fb-4e21-a51c-24e862b7953e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LLM Inference Server URL\n",
    "inference_server_url = \"http://llm.ic-shared-llm.svc.cluster.local:8000\"\n",
    "\n",
    "# LLM definition\n",
    "llm = VLLMOpenAI(           # we are using the vLLM OpenAI-compatible API client. But the Model is running on OpenShift, not OpenAI.\n",
    "    openai_api_key=\"EMPTY\",   # and that is why we don't need an OpenAI key for this.\n",
    "    openai_api_base= f\"{inference_server_url}/v1\",\n",
    "    model_name=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    top_p=0.92,\n",
    "    temperature=0.01,\n",
    "    max_tokens=512,\n",
    "    presence_penalty=1.03,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b950bc-4d73-49e5-a35b-083a784edd50",
   "metadata": {},
   "source": [
    "The **template** we will use is now formatted for this specific summarization task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bb7517-faa2-43ed-a95d-835de975f916",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template=\"\"\"<s>[INST]\n",
    "You are a helpful, respectful and honest assistant.\n",
    "Always assist with care, respect, and truth. Respond with utmost utility yet securely.\n",
    "Avoid harmful, unethical, prejudiced, or negative content. Ensure replies promote fairness and positivity.\n",
    "I will give you a text that you must summarize as best as you can.\n",
    "\n",
    "### TEXT:\n",
    "{input}\n",
    "\n",
    "### SUMMARY:\n",
    "[/INST]\n",
    "\"\"\"\n",
    "PROMPT = PromptTemplate(input_variables=[\"input\"], template=template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbe2119-2128-4432-aed1-126e9c8c034f",
   "metadata": {},
   "source": [
    "And we can now create the **conversation** object that we will use to query the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6d9f32-d4ae-4c2f-b513-d520413d2cc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conversation = LLMChain(llm=llm,\n",
    "                        prompt=PROMPT,\n",
    "                        verbose=False\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849fbd67-220c-4a02-8e4e-7e0d1aa91588",
   "metadata": {},
   "source": [
    "We are now ready to query the model!\n",
    "\n",
    "In the `claims` folder we have JSON files with examples of claims that could be received. We are going to read those files, display them, then the summary that the LLM made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca714bca-7cec-4afc-b275-fa389c05a993",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read the claims and populate a dictionary\n",
    "claims_path = 'claims'\n",
    "onlyfiles = [f for f in listdir(claims_path) if isfile(join(claims_path, f))]\n",
    "\n",
    "claims = {}\n",
    "\n",
    "for filename in onlyfiles:\n",
    "    # Opening JSON file\n",
    "    with open(os.path.join(claims_path, filename), 'r') as file:\n",
    "        data = json.load(file)\n",
    "    claims[filename] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac009d5-d558-4258-9735-4fb0de46c309",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for filename in onlyfiles:\n",
    "    print(f\"***************************\")\n",
    "    print(f\"* Claim: {filename}\")\n",
    "    print(f\"***************************\")\n",
    "    print(\"Original content:\")\n",
    "    print(\"-----------------\")\n",
    "    print(f\"Subject: {claims[filename]['subject']}\\nContent:\\n{claims[filename]['content']}\\n\\n\")\n",
    "    print('Summary:')\n",
    "    print(\"--------\")\n",
    "    summary_input = f\"Subject: {claims[filename]['subject']}\\nContent:\\n{claims[filename]['content']}\"\n",
    "    conversation.predict(input=summary_input);\n",
    "    print(\"\\n\\n                          ----====----\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e28a5b0-6c93-42ba-84dd-42e17746d11d",
   "metadata": {},
   "source": [
    "You can come back to this notebook at section 3.7 for some optional exercises if you want."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
