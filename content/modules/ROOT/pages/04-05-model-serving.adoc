= Model Serving
include::_attributes.adoc[]

. At this point, we need to deploy the model into RHOAI model serving.
. Look at the information used to create the Data Connection that was used for the pipeline server.

. Re-create another data connection, with identical information, but change the bucket name from `userX` to 'models'

== Create a Data Connection

* In your project, create a data connection that maps to the shared minio.
* Here is the info you need to enter:
** Name:
[.lines_space]
[.console-input]
[source, text]
[subs=attributes+]
Shared Minio - model
** Access Key:
[.lines_space]
[.console-input]
[source, text]
[subs=attributes+]
{minio-user}
** Secret Key:
[.lines_space]
[.console-input]
[source, text]
[subs=attributes+]
{minio-pass}
** Endpoint:
[.lines_space]
[.console-input]
[source, text]
[subs=attributes+]
{minio-endpoint}
** Region:
[.lines_space]
[.console-input]
[source, text]
[subs=attributes+]
none
** Bucket:
[.lines_space]
[.console-input]
[source, text]
[subs=attributes+]
models

* The result should look like:
+
[.bordershadow]
image::04/model-data-connection.png[model connection]

== Create a Model Server

In your project create a model server.

- Click 'Add server'

[.bordershadow]
image::04/add-model-server.png[]

Here is the info you need to enter:

** Model server name:
[.lines_space]
[.console-input]
[source, text]
[subs=attributes+]
My first Model Server
** Serving runtime:
[.lines_space]
[.console-input]
[source, text]
[subs=attributes+]
OpenVINO Model Server
** Number of model server replicas to deploy:
[.lines_space]
[.console-input]
[source, text]
[subs=attributes+]
1
** Model server size
[.lines_space]
[.console-input]
[source, text]
[subs=attributes+]
Standard
** Accelerator
[.lines_space]
[.console-input]
[source, text]
[subs=attributes+]
None
** Model route
[.lines_space]
[.console-input]
[source, text]
[subs=attributes+]
unchecked
** Token authorization
[.lines_space]
[.console-input]
[source, text]
[subs=attributes+]
unchecked


The result should look like:

[.bordershadow]
image::04/add-model-server-config.png[]

You can click on **Add** to create the model server.

== Deploy the Model

In your project, under 'Models and model servers' select 'Deploy model'.

* Click 'Deploy model'
+
[.bordershadow]
image::04/select-deploy-model.png[]

* Here is the information you will need to enter:

** Model name:
[.lines_space]
[.console-input]
[source, text]
[subs=attributes+]
My first Model
** Model server
[.lines_space]
[.console-input]
[source, text]
[subs=attributes+]
My first Model Server
** Model server - Model framework
[.lines_space]
[.console-input]
[source, text]
[subs=attributes+]
onnx-1
** Existing data connection - Name
[.lines_space]
[.console-input]
[source, text]
[subs=attributes+]
Shared Minio - model
** Existing data connection - Path
[.lines_space]
[.console-input]
[source, text]
[subs=attributes+]
accident/

The result should look like:

[.bordershadow]
image::04/deploy-a-model.png[]

Click on "Deploy". If the model is successfully deployed you will see its status as green after 15 to 30 seconds.

[.bordershadow]
image::04/model-deployed-success.png[]

We will now confirm that the model is indeed working by querying it!

== Using the served Model

Once the model is served, we can use it directly as an endpoint that can be queried.

First, we need to get the URL of the model server. To do this, click on the **Internal Service** link under the **Inference endpoint** column.

This will open a new tab with the URL of the model server.

[.bordershadow]
image::04/inference-url.png[]

Note or copy the **RestUrl**, which should be something like `http://modelmesh-serving.{user}:8008`.

We will now use this URL to query the model.

- In your running workbench, navigate to the folder `lab-materials/04`.
- Look for (and open) the notebook called `04-05-model-serving.ipynb`.
- Execute the cells of the notebook, and ensure you understand what is happening.



